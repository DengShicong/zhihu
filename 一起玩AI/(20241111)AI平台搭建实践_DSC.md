# AI平台搭建实践

 **Author:** [DSC]

 **Link:** [https://zhuanlan.zhihu.com/p/6251002380]

AI学习工作的过程中，偏软件开发的专业人士基于“开箱即用”的场景，忽略算力硬件及网络技术相关的技术细节；偏硬件(如CPU/GPU等）知识架构的人员往往只关注个人应用场景，对涉及AI大模型的底层系统、软件、模型框架不了解。

此文档是基于笔者个人的实践经验，结合了对硬件及软件技术综合梳理及记录，如硬件底座，GPU类型选择，大模型技术需求，如何实现在Ubuntu内安装Nvidia驱动及AI环境搭建等，可供读者参考搭建一套支持AI大模型推理的系统平台。

## 研究背景  
人工智能（AI）技术的迅速发展，使得各种AI软件和工具层出不穷。这些现有的软件提供了丰富的功能，帮助工程师和研究人员在开发、训练、测试和部署AI模型时事半功倍。例如，许多云端AI平台提供了强大的计算资源和简化的工作流程，让用户可以快速启动项目，并轻松实现模型的部署。

然而，这些现有平台/软件也存在一些不可忽视的缺陷。首先，云端AI平台虽然便捷，但成本较高，特别是对于长期、大规模的训练任务，费用可能难以控制。其次，云服务的使用也可能面临数据隐私和安全风险，尤其是在处理敏感数据时。此外，由于平台的封闭性和依赖性，用户在自定义环境和深入优化时常常受到限制，无法完全掌控整个开发和测试过程。

举个实际例子，当2024年6月25日，OpenAI突然宣布停止对部分国家和地区提供API服务，各大AI厂商都马上开展免费（优惠）的活动，阿里云百炼宣布将为OpenAI API用户提供最具性价比的中国大模型替代方案，并为中国开发者提供2200万免费tokens和专属迁移服务；通义千问GPT4级主力模型Qwen-plus在阿里云百炼上的调用价格为0.004元/千tokens，仅为GPT-4的50分之一，但如果是大量的AI服务，始终是需要昂贵的使用费用。

而本地模型的好处就是可以无限制使用token，通过实验环境不停的喂数据，让AI自己去训练/推理，直到给出想要的结果。

各种限制促使越来越多的企业/工程师转向本地搭建AI平台。本地搭建的AI平台不仅可以降低长期的使用成本，还能提供更高的灵活性和控制权，使开发者能够根据项目的具体需求自定义环境，进行深度优化。同时，本地平台能够更好地保障数据的隐私和安全，特别适用于需要严格数据保护的应用场景。



---

### 整机（服务器）or 组装机  
如果资金预算充裕，并且目标为4卡（GPU），8卡的AI训练/推理需求。请选择购买专业的服务器平台，全新或二手（洋垃圾组件、准系统）均可，主要考虑以下因素：

* 4卡/8卡的安放需要更大的空间尺寸，无论是硬件安装需求（PCIE插槽)还是专业散热设计，专业服务器更适合；
* 4卡/8卡平台要求的功率，至少2000 Watts起步，甚至双（多）电源，专业服务器平台在电路/电器上更安全；
* 系统的稳定性和可靠性方面，既然投入了4卡/8卡，固然是希望能最大限度，最长工作时长的持续运作，专业服务器（DELL ,HP，Lenovo等）厂商都有严格的出厂测试；  
如果资金预算有限，仅仅是单卡（GPU）/双卡，可以考虑自选组件来搭建。
* 专业服务器基本是机架式，空间占用大（不可能使用PCIE转接卡做横插卡架构的1 U设计），而组转机可以用立式机箱，落地摆放，空间利用灵活；
* 专业服务器的一些可靠性设计（如CPU的被动散热），往往采用万转（暴力）风扇做风道散热，导致噪声大，不利用办公场景（没有专业机房）使用；
* 造价低，因为零部件的选择余地大，而且部分可以利旧（如硬盘等）

备注：需要具备一定的硬件基础知识，例如主板主芯片与CPU的适配度，PCIE通道的数量对性能的影响，如何组建合理的散热风道，GPU的涡轮散热和风扇散热选择，甚至多卡互联的NVLink桥接,NVSwitch的技术等；

以下为本次搭建的深度学习主机，可供参考。



| 序号 | 组件 | 描述 | 备注 | 造价 |
| --- | --- | --- | --- | --- |
| 1 | 机箱 | 追风者（Phanteks）614 PC | EATX 尺寸机箱 |  |
| 2 | 电源 | 追风者850 Watts 全模组金牌电源 | 偏保守，可改选全新巨龙电源 |  |
| 3 | 主板 | 联想RD450X ，X99平台，支持双路E5 v3/v4 ,2个万兆电口，IPMI管理，最大512G内存（16\*32G)，PCIE 3.0 (2个X16 ,4个X8） ，板载10个SATA 3.0 接口。 | 该款型是联想为某度，某讯提供定制款，市场存量大， | ￥350\*1 |
| 4 | CPU | Intel XEON E5-2666v3,22纳米，10核/20线程，TDP 135W | 2个（双路） | ￥50\*2 |
| 5 | 风扇 | CPU 散热：6热管双风扇机箱前后风道：1（前入风）+2（增压）+1（后出风）机箱上下风道：3（上出风）+1（下入风） | 选用静音风扇（没有必要用Noctua等奢侈品牌,没有必要用水冷），尽量背部走线，避免影响风道，可适当用胶钉固定安装，避免与机箱共振 |  |
| 6 | GPU | Nvidia 2080Ti （22G显存） | 备注：因大模型对GPU显存有要求，选择了魔改款（11G 改22G） | ￥3000\*1 |
| 7 | 内存 | ECC DDR4-2133 , 16G\*8 | 组成4通道X99支持DDR3/DDR4，推荐DDR4，扩展性好 | ￥85\*8 |
| 8 | 硬盘 | SSD SATA ,1T SATA3 ，1T | 可利旧处理，在AI训练/推理时，NVMe M.2硬盘容易过热掉盘，选择SATA 接口SSD 可实现读写速度和高温的平衡； |  |
| 8 | 阵列 | LSI 2308 | 可选，需结合主板现有硬盘接口数量和机箱硬盘位而定 | ￥50 |
| 9 | 配件 | 显示器，键盘，鼠标、 | 可利旧处理，亦可系统安装结束后，开启远程桌面进行操作，无需显示器，键盘，鼠标 |  |

### 机箱选型经验  
* 空间尺寸要大：GPU的尺寸比较大，从空间占用和散热角度出发，机箱越大越好，优先考虑选择Extended ATX（扩展型ATX）主板所适配的机箱。EATX主板主要用于高性能PC整机、入门式工作站等领域，它通常用于双处理器和标准ATX主板上无法胜任的服务器。
* 散热位设计合理：因为后续选用了双路CPU （Intel E5-2666v3), 必须有足够的风冷散热安装位置。
* 静音设计：因为深度学习主机，电源功耗大，机箱内部风扇众多（CPU ,GPU ，机箱散热），如果要在办公环境下摆放使用，对机箱的隔音有要求（隔音棉或者机箱厚度）。
* 硬盘位：部分机箱号称EATX尺寸，但设计不合理，硬盘安装位置和EATX主板有冲突，会导致在主板选型和硬盘位上做取舍，但大概率会优先考虑主板的倾向因素。  
实例示范：
* 追风者614PC机箱和安钛克的P101S静音，均为EATX机箱，用料选材都不错，但安钛克101S如果要安装多盘位硬盘，只能选ATX主板。而服务器主板，尤其是双路CPU主板，都是EATX尺寸。
* 为图省事，可能会选择服务器准系统（机箱+电源+主板+CPU），但要考虑是否有机架来摆放，而且也要考虑风扇的噪音（可以用降速线降噪，或者整体风扇改造）；
* 其他的一些低端品牌的EATX主板，可能空间尺寸足够，但用料太差，当机箱风扇数量比较多，很容易导致共振，既影响硬盘寿命，也会有噪声。

### 电源选型经验  
主机的整体功率是最重要的考量因素：

* 整机的功耗最大极限值，如主要部件：CPU （E5 2666-V3）TDP 135Wats ，GPU 如Nvidia 2808 Ti 不超频为250Watts,单个SATA硬盘10Watts （SAS硬盘功率略大），
* 电源的额定功率和转换率，可以在超能网（[https://www.expreview.com/](https://www.expreview.com/)）了解各品牌，各类型电源的差异，如金牌电源：负载功率20%效率为87%，负载功率50%效率为90%，负载功率100%效率为87%。
* 接口数量及扩展性：考虑到双路CPU （8Pin供电\*2），GPU （6+2 Pin) ,1-2个，是否需要支持ATX 3.0 (PCIE 5.0）等
* 不建议买二手电源，例如矿机用的巨龙1250 Watts

整机组装完毕以后，建议用【图吧工具箱】一键烤机，配合功率计插座查看最大功耗值。

### 主板选型经验  
首先明确主板芯片架构，它和适配的CPU，PICE通道有关。

以下为主板选型的经验：

* 品牌：优先选择主流服务器厂商的产品，例如Supermicro(超微，缺点，贵），联想，浪潮等，但不要选服务器异型主板，他们的布局不适合标准机箱的安装，会导致电源，PCIE卡无法安放；
* 选择Intel路线：双路CPU ，性价比最好的是X99平台 。因为E5 v3 v4的大幅度降价，X99支持通过打补丁方式锁全核心睿频，从而提升CPU的单核性能（原理是利用了E5 v3的一个硬件BUG，实际上和超频略有不同，即占用更高时，频率才会更高）。AMD Epyc一代/二代虽然核心数相同，甚至更多，但板+U购买成本高，而且在兼容性方面也欠缺（相对INTEL而言）
* PCIE的技术规格：X99平台只支持PCIE 3.0 ，注意要有PCIE X16的插槽，满足GPU的全尺寸的安装（PCIE 3.0 X16,PCIE 3.0 X8 在带宽上有差异，但在NVIDIA 2080 Ti这个级别的GPU的算力吞吐的性能差异不到5% ，
* PCIE 插槽的布局 ，单卡平台，GPU插在离CPU最近的插槽上才能实现满速PCIE 3.0 X16，如果是多卡（双卡）平台，可能由于PCIE 插槽挨得过近，而导致无法实现多GPU卡的安装。不建议把PCIE 3.0 X16的GPU 卡插在PCIE 3.0 X8的插槽上，
* 不要买号称6卡，8卡的挖矿平台主板，他们基本上是B75/B85魔改，为了散热，PCIE插槽非标准间隔（6.5/7.0cm)，如果要走2080 ti多卡路线需要考虑和NVLINK 桥接卡兼容适配性（NVLINK BRIDGE提供3槽位和4槽位两个版本，区别在于尺寸间隙，前者双卡模式下两卡间隙预留1个槽位，而4槽位为2个槽位的间隙 ，8cm，）

![]((20241111)AI平台搭建实践_DSC/v2-112b4c02eeb74716334aa5f2fdc6f9f9_1440w.jpg)  


网上某宝/某鱼的多卡主机

  
  
备注1：联想的RD450X是一款非常不错的X99 ,双路CPU主板，优点是市场存量大（该主板是联想转为某互联网公司定制），而且有万兆网络接口（注意，有部分型号是千兆接口） ，硬盘接口丰富，板载2个SATA +3个Mini SAS (1分4）；但他的不足在于只适合单GPU（主板两个PCIE 3.0 X16紧挨在一起，无法安装双卡），不建议用PCIE 3.0 X8的插槽配2块GPU卡（性能低而且未必能用，虽然有技术资料测试性能下降仅5%）；

![]((20241111)AI平台搭建实践_DSC/v2-e057861ed0f932e102790e811b8273ec_1440w.jpg)  


联想RD450X主板实物图

  
  


---

![]((20241111)AI平台搭建实践_DSC/v2-aca28e93609e2d2f1395fd98e2fe8630_1440w.jpg)  


联想RD450X主板技术参数

  
  
备注2：如果是双卡或者4卡GPU平台，推荐用超微7048GR-TR/浪潮NF5568M4主板

备注3：如果是6卡或者8卡GPU平台，推荐用超微4028GR-TR/浪潮NF5588M4主板

备注4：如果选择4卡/8卡GPU平台，建议选择涡轮散热的GPU，配合机箱强力散热风扇，因为两/叁散热风扇版的GPU 不适合安装在紧凑空间，不利于进/吸风。

![]((20241111)AI平台搭建实践_DSC/v2-9d86b48eb2f98da1239eb0df80155ffd_1440w.jpg)  
图2-4某品牌多GPU服务器内部结构

![]((20241111)AI平台搭建实践_DSC/v2-101a0265e6caeb4199dc4afd72fef0b8_1440w.jpg)  


某品牌多GPU服务器背部PCIE挡板示意

  
  
### CPU选型经验  
尽管CPU并没有直接参与到深度学习模型的计算中，但它需要具备比模型训练吞吐量更强的数据处理能力。通常情况下，每块GPU都会被分配固定数量的CPU逻辑核心，以确保在多线程异步数据读取时能够提供足够的处理能力。

毫无疑问在AI计算领域AMD EPYC更被青睐，EPYC 处理器提供了更多的核心和线程，能够支持并行处理，这对于需要大量数据预处理、模型训练和推理的 AI 任务非常关键。更多的核心意味着能够更好地处理多线程的异步数据读取、模型并行计算和大规模数据集的预处理。大内存与支持PEIC 4.0 提供更高的带宽和更快的数据传输速度。但是价格比目前的E5v3 贵得多，单CPU之间的价格可能有十倍差距，配套的主板同样如此。



| 特征 | Intel Xeon E5 v3 | AMD EPYC |
| --- | --- | --- |
| 架构 | Haswell/Broadwell (22nm/14nm) | Zen, Zen 2, Zen 3, Zen 4 (14nm/7nm/7nm) |
| 核心数/线程数 | 4 - 18 核心/8 - 36 线程 | 8 - 64 核心/16 - 128 线程 |
| 单线程性能 | 优秀（高主频） | 良好，但低于至强 E5 v3 |
| 多线程性能 | 中等，适合较小规模并行任务 | 优秀，适合大规模并行计算 |
| 内存支持 | DDR4, 4通道, 最大1.5 TB内存 | DDR4, 8 通道, 最大4 TB内存 |
| PCIe 通道数 | 40 条 PCIe 3.0 通道 | Zen: 64条 PCIe 3.0 通道 Zen 2, Zen 3, Zen 4: 128 条 PCIe 4.0 通道 |
| PCIe 版本 | PCIe 3.0 | PCIe 3.0/PCIe 4.0 |
| 扩展性 | 中等（较少的PCIe通道，较低的内存带宽） | 高（更多PCIe通道，高内存带宽） |
| 功耗 | 85W - 160W | 120W - 280W |

* E5 [https://zhida.zhihu.com/search?q=2666v3&zhida\_source=entity&is\_preview=1](https://zhida.zhihu.com/search?q=2666v3&zhida\_source=entity&is\_preview=1):10核心20线程，默认频率2.9GHz 全核鸡血后能到3.5GHz ，价格在45-60元之间;
* E5 2696v3 :18核心 36线程， 默频2.3GHz 全核鸡血后能到 3.8GHz，撞功耗墙会降频。价格在250-300元之间;
* E5 v4版：例如E5 2696V4 （22核心44线程，默认频率2.3GHz ，睿频3.6GHz，价格在1000-1100元之间，但由于不能上[https://zhida.zhihu.com/search?q=鸡血补丁&zhida\_source=entity&is\_preview=1](https://zhida.zhihu.com/search?q=鸡血补丁&zhida\_source=entity&is\_preview=1),性价比极低;

### GPU选型经验  
GPU的选择与AI的技术应用方向有关，例如是AI出图，文生视频，还是各种本地LLM训练或推理。因为要满足做本地LLM训练推理的用途，这意味着更高的计算和数据存储需求。但对于个人学习研究，必须考虑资金投入的科学性。

学习AI目前只能选N卡，受限于CUDA 生态系统，深度学习AI领域在很大程度上倾向于选择NVIDIA的GPU，CUDA因其成熟稳定、支持库丰富、性能优化良好而被广泛采用，而ROCm（AMD）作为较新的开源平台，尽管拥有良好的开放性和跨平台特性，但在生态系统、性能优化和应用支持方面与CUDA相比尚有不足。

NVIDIA GPU硬件架构的主要有如下分类：



| 序号 | 架构 | 发布时间 | 代表产品 | 特点 |
| --- | --- | --- | --- | --- |
| 1 | Kepler | 2012年 | GeForce GTX 700 系列, Tesla K80 | 引入了双精度浮点性能优化，适用于科学计算，但在深度学习领域的应用较为有限。重点提升了能效比，虽然相比后续架构，其在深度学习中的表现较弱。 |
| 2 | Maxwell | 2014年 | GeForce GTX 900 系列, Tesla M40 | 首次引入了统一内存架构，使得 CPU 和 GPU 之间的内存共享更加高效。虽然更注重图形处理，但也为深度学习中的数据管理提供了更好的基础。 |
| 3 | Pascal | 2016年 | GeForce GTX 10 系列（如 GTX 1080, GTX 1070）, Tesla P100 | 引入了对半精度浮点（FP16）的支持，增强了深度学习计算能力。引入 NVLink 技术，支持多 GPU 高速互联，适合大规模模型训练。 |
| 4 | Volta | 2017年 | GeForce GTX 10 系列（如 GTX 1080, GTX 1070）, Tesla P100 | 首次引入 Tensor Core，大幅提升深度学习任务中的矩阵乘法效率，尤其是在混合精度计算中。支持 HBM2 内存，提供了更高的内存带宽和容量，适合训练大型深度学习模型。 |
| 5 | Turing | 2018年 | GeForce RTX 20 系列（如 RTX 2080, RTX 2070）, Quadro RTX 8000 | 引入光线追踪核心（RT Core），主要用于图形渲染，但在 AI 领域的某些任务中也有应用。改进的 Tensor Core，支持更多类型的深度学习运算，如 INT8 和 INT4 运算，提升了推理性能。 |
| 6 | Ampere | 2020年 | GeForce RTX 30 系列（如 RTX 3090, RTX 3080）, NVIDIA A100 | 第三代 Tensor Core，进一步提升深度学习模型的训练和推理性能。引入多实例 GPU (MIG) 功能，提高了资源利用率，特别是在多租户环境中。 |
| 7 | Hopper | 2022年 | NVIDIA H100 | 第四代 Tensor Core，专为处理稀疏性和动态形状的张量计算优化，显著提升深度学习效率。引入 Transformer Engine，优化了自然语言处理任务中的 Transformer 模型性能。 |
| 8 | Hopper-Next | 2025 | RTX 50 系列 | AI和HPC领域的技术衍生 |

备注1：从 Kepler 到 Hopper，NVIDIA 的 GPU 架构随着时间推移不断发展，每一代架构都引入了新的技术和优化，逐步提高了性能，特别是在深度学习领域的应用中。

备注2：其他架构

* 寒武纪 MLU 200系列加速卡。 暂不支持模型训练。使用该系列加速卡进行模型推理需要量化为int8进行计算。 并且需要安装适配寒武纪MLU的深度学习框架。
* 华为 Ascend 系列加速卡。 支持模型训练及推理。但需安装MindSpore框架进行计算。

2、选那一款N卡比较合适

目前企业的一般选择是，最先进最新型号的GPU显卡，会用于训练。上一代或更久远的GPU显卡用于推理（推理一般会关注FP32、FP16 和 INT8 浮点性能参数差异和显存，这些差异影响模型的准确性、速度和资源使用）。

1. FP32 (单精度浮点)  
最精确的浮点表示，提供最高的数值准确性。计算速度相对较慢，尤其是在大型模型上。适用于适合对数值精度要求较高的任务，如一些复杂的模型或需要高精度输出的应用。在某些情况下，可能会导致较高的延迟和较低的吞吐量。
2. FP16 (半精度浮点)  
提供较高的性能和较低的内存占用，通常是 FP32 性能的 2 倍（在支持的硬件上）。适用于大多数深度学习推理任务，尤其是当模型经过适当的量化和优化时。适用于许多现代深度学习框架和硬件上，FP16 可以保持足够的精度与性能平衡。特别适合需要实时推理的应用，如视频处理和在线服务。
3. INT8 (整数)

提供最高的推理性能，通常是 FP16 性能的 2-4 倍（在支持的硬件上），显著降低了内存消耗。推理速度快，延迟低，适合高吞吐量的应用。适合于推理阶段，尤其是当模型经过量化并且能容忍较小的精度损失时。广泛应用于边缘计算和嵌入式设备，因为其低功耗特性。

总结：FP32 提供最高的准确性，但推理速度较慢，适合对精度要求极高的场景。FP16 兼顾性能和准确性，是许多深度学习推理任务的良好选择。INT8 在速度和资源效率上表现最佳，但对于某些模型，可能会带来较大的精度损失。在选择用于推理的浮点精度时，需根据具体应用需求、模型特性和可接受的精度损失进行权衡。

以下是N卡的技术参数说明



| 型号 | 显存 | 单精(FP32) | 半精(FP16) | 说明 |
| --- | --- | --- | --- | --- |
| Tesla P40 | 24GB | 11.76 T | 11.76 T | 比较老的Pascal架构GPU，对于cuda11.x之前且对大显存有需求的算法是非常不错的选择 |
| TITAN Xp | 12GB | 12.15 T | 12.15 T | 比较老的Pascal架构GPU，用作入门比较合适 |
| 1080 Ti | 11GB | 11.34 T | 11.34 T | 和TITANXp同时代的卡，同样适合入门，但是11GB的显存偶尔会比较尴尬 |
| 2080Ti | 11GB | 13.45 T | 53.8 T | 图灵架构GPU，性能还不错，老一代型号中比较适合做混合精度计算的GPU。性价比高 |
| 3060 | 12GB | 12.74 T | 约24T | 如果1080Ti的显存正好尴尬了，3060是不错的选择，适合新手。需要使用cuda11.x |
| V100 | 16/32GB | 15.7 T | 125 T | 老一代专业计算卡皇，半精性能高适合做混合精度计算 |
| A4000 | 16GB | 19.17 T | 约76T | 显存和算力都比较均衡，适合进阶过程使用。需要使用cuda11.x |
| 3080Ti | 12GB | 34.10 T | 约70T | 性能钢炮，如果对显存要求不高则是非常合适的选择。需要使用cuda11.x |
| 3090 | 24GB | 35.58 T | 约71T | 可以看做3080Ti的扩显存版。性能和显存大小都非常够用，适用性非常强，性价比首选。需要使用cuda11.x |
| A100 SXM4 | 40/80GB | 19.5 T | 312 T | 新一代专业计算卡皇，除了贵没缺点。显存大，非常适合做半精计算，因为有NVLink加持，多卡并行加速比非常高。需要使用cuda11.x |
| 4090 | 24G | 82.58 T | 165.2 T | 新一代游戏卡皇，除显存比较小和多机多卡并行效率低的缺点外，性价比非常高 |



---

### LLM大模型需要什么样GPU？  
**显存并不决定运行速度，显存决定能不能跑模型。**

Meta发布了Llama 3.1的一系列开源模型，在多项基准测试中，超越了GPT-4o和Claude 3.5 Sonnet等闭源的商业大模型。开源大模型性能越来越强，可以满足很多私有化部署的业务场景。不同于商业模式的服务调用模式，需要自己搭建环境，部署模型。有些时候还要自己微调模型，以更好适应特有的场景。

LLM 的特点是计算要求非常高，具有数十亿个参数，并且需要对数 TB 的数据进行训练。

**那么多大显存才能跑主流的大模型呢？**

推理：参数数量 \* 精度（通常为 2 或 4 个字节）

训练：4-6倍推理资源

10亿个参数则将占用20亿个字节，或者说10亿个字节等于1GB，那么1B个参数占用2GB的内存。100B参数就需要占用200GB内存。这是一个近似值，因为1 KB不等于1,000字节，而是1,024字节。通过这种简单的方法可以大概评估内存的占用。

以meta-llama/Meta-Llama-3.1-70B-Instruct为例，70.6B，16位精度，那大概是142G的GPU内存，需要两个80G的H100就可以。而搭载H100的服务器（8张卡）在国内的报价一度高达人民币300多万元（超过42万美元），比官方售价28万至30万美元高出了50%左右。可见AI领域，硬件资源投入非常昂贵，例如，训练 GPT-4 的成本估计约为 1 亿美元。

![]((20241111)AI平台搭建实践_DSC/v2-93d765aef00499b3ebf5403a76fab061_1440w.jpg)  
![]((20241111)AI平台搭建实践_DSC/v2-11f08da67d3ce6b3ed6b2dc751209a0b_1440w.jpg)  
如果不会计算，可以有取巧方式，在下述网站直接得出各种主流模型的显存需求结果

[https://llm-system-requirements.streamlit.app/](https://llm-system-requirements.streamlit.app/)

![]((20241111)AI平台搭建实践_DSC/v2-a65e3f1d58621a50c29c0af126876654_1440w.jpg)  
可以看到仅仅是Meta-Llama-3-8B ，就需要19.46G 显卡，要满足这个要求，只能选择RTX 3090-24G（￥6400-7600 ，不排除为矿卡） ，RTX 4090D-24G （某东，￥14000-18999）

我们属于个人学习，因此选择了一款曾经的一代卡皇‌[https://www.baidu.com/s?wd=RTX 2080 Ti&usm=1&ie=utf-8&rsv\_pq=afbef028000b5ddd&oq=2080ti改22g显存能买吗&rsv\_t=4c22cM0a1ZbPyryY1LTniDY+PTc7xa0U4/8PLUxwpjsaUoWz9zRDvuKGJdQ&sa=re\_dqa\_generate](https://www.baidu.com/s?wd=RTX 2080 Ti&usm=1&ie=utf-8&rsv\_pq=afbef028000b5ddd&oq=2080ti改22g显存能买吗&rsv\_t=4c22cM0a1ZbPyryY1LTniDY+PTc7xa0U4/8PLUxwpjsaUoWz9zRDvuKGJdQ&sa=re\_dqa\_generate) 魔改版（22G）。

这张消费级GPU支持多卡互联，例如可以通过4卡（22G\*4)去跑34B的模型。

‌[https://www.baidu.com/s?wd=RTX 2080 Ti&usm=1&ie=utf-8&rsv\_pq=afbef028000b5ddd&oq=2080ti改22g显存能买吗&rsv\_t=4c22cM0a1ZbPyryY1LTniDY+PTc7xa0U4/8PLUxwpjsaUoWz9zRDvuKGJdQ&sa=re\_dqa\_generate](https://www.baidu.com/s?wd=RTX 2080 Ti&usm=1&ie=utf-8&rsv\_pq=afbef028000b5ddd&oq=2080ti改22g显存能买吗&rsv\_t=4c22cM0a1ZbPyryY1LTniDY+PTc7xa0U4/8PLUxwpjsaUoWz9zRDvuKGJdQ&sa=re\_dqa\_generate)是一款在2018年发布的旗舰级显卡，采用了11GB的‌[https://www.baidu.com/s?wd=GDDR6&usm=1&ie=utf-8&rsv\_pq=afbef028000b5ddd&oq=2080ti改22g显存能买吗&rsv\_t=4c22cM0a1ZbPyryY1LTniDY+PTc7xa0U4/8PLUxwpjsaUoWz9zRDvuKGJdQ&sa=re\_dqa\_generate](https://www.baidu.com/s?wd=GDDR6&usm=1&ie=utf-8&rsv\_pq=afbef028000b5ddd&oq=2080ti改22g显存能买吗&rsv\_t=4c22cM0a1ZbPyryY1LTniDY+PTc7xa0U4/8PLUxwpjsaUoWz9zRDvuKGJdQ&sa=re\_dqa\_generate)显存，拥有4352个‌[https://www.baidu.com/s?wd=CUDA核心&usm=1&ie=utf-8&rsv\_pq=afbef028000b5ddd&oq=2080ti改22g显存能买吗&rsv\_t=0e5dkSyOeDsdT41/StHea7/UYXxFZx+SmyZe37z3ZTm3eDq6yDvngpWHlNA&sa=re\_dqa\_generate](https://www.baidu.com/s?wd=CUDA核心&usm=1&ie=utf-8&rsv\_pq=afbef028000b5ddd&oq=2080ti改22g显存能买吗&rsv\_t=0e5dkSyOeDsdT41/StHea7/UYXxFZx+SmyZe37z3ZTm3eDq6yDvngpWHlNA&sa=re\_dqa\_generate)和352bit的显存位宽。原版RTX2080ti显卡配备的显存，通常包含11个1GB的显存芯片。为了增加显存容量，魔改者会替换这些显存芯片，使用更大容量的显存芯片，比如2GB的芯片，从而将总显存提升至22GB。RTX2080ti显卡能够魔改至22G显存，而其他显卡不行的原因，主要在于RTX2080ti是基于NVIDIA的Turing架构，它与NVIDIA的专业级显卡共享相同的核心，因此在驱动和硬件兼容性方面可以进行魔改。22G显存对于需要处理大量数据和复杂场景的专业工作，如3D渲染、AI模型训练或高分辨率视频处理有更大的适用。

至少我们通过‌[https://www.baidu.com/s?wd=RTX 2080 Ti&usm=1&ie=utf-8&rsv\_pq=afbef028000b5ddd&oq=2080ti改22g显存能买吗&rsv\_t=4c22cM0a1ZbPyryY1LTniDY+PTc7xa0U4/8PLUxwpjsaUoWz9zRDvuKGJdQ&sa=re\_dqa\_generate](https://www.baidu.com/s?wd=RTX 2080 Ti&usm=1&ie=utf-8&rsv\_pq=afbef028000b5ddd&oq=2080ti改22g显存能买吗&rsv\_t=4c22cM0a1ZbPyryY1LTniDY+PTc7xa0U4/8PLUxwpjsaUoWz9zRDvuKGJdQ&sa=re\_dqa\_generate) 魔改版（22G）能运行8B模型，通过某些开源压缩算法能运行更多模型。



---

### 多卡互联技术NVLink和Nv Switch  
Nvlink是Nvidia 的芯片互联技术，通常用于GPU之间或者GPU到CPU

[https://www.nvidia.cn/design-visualization/nvlink-bridges/](https://www.nvidia.cn/design-visualization/nvlink-bridges/)

Nv Switch是基于Nvlink技术的芯片或者类似交换机的设备。在主机内部NvSwitch就是芯片，跨主机互联时NVSwitch就是搭载Nv switch芯片的独立设备。

![]((20241111)AI平台搭建实践_DSC/v2-271738acf6218deacb20db3faec5b819_1440w.jpg)  
![]((20241111)AI平台搭建实践_DSC/v2-dbae8e2117b78777c5205db7fb812757_1440w.jpg)  
备注：Nvidia 2080 ti支持多卡互联（Nvlink) ,以实现大容量模型的需求（如405B），但由于联想RD450X 的PCIE 3.0 X16插槽不足，只能留着以后更换主板再实现。

![]((20241111)AI平台搭建实践_DSC/v2-a0256ad08b14c7529fd5c92c0eb2fb5e_1440w.jpg)  
### 系统需求分析  
在搭建深度学习和人工智能（AI）平台时，使用 Ubuntu 系统已经成为很多研究机构和企业的首选。虽然 CentOS、Rocky Linux 等其他 Linux 发行版也具备相似的功能，但 Ubuntu 在深度学习和 AI 开发方面具有明显的优势。以下是具体的分析，涵盖技术支持、社区资源、兼容性和性能等方面，并附带一些相关引用。

* **广泛的社区和企业支持**  
Ubuntu 拥有庞大的用户群体和活跃的社区支持。这意味着遇到问题时，开发者可以更快地找到解决方案。此外，Canonical（Ubuntu 的开发商）提供了强大的企业支持，包括长期支持版本（LTS），这对需要稳定环境的企业用户尤为重要。
* **与主流深度学习框架的兼容性**  
Ubuntu 是 TensorFlow、PyTorch、Keras 等深度学习框架的首选开发和运行环境。许多框架的官方文档和教程都默认基于 Ubuntu，开发者可以快速上手并减少环境配置上的麻烦。
* **优化的驱动支持**  
Ubuntu 提供了优秀的硬件兼容性，尤其是在 NVIDIA GPU 驱动和 CUDA 的支持上非常出色。深度学习通常依赖 GPU 的加速能力，Ubuntu 官方仓库中提供的 NVIDIA 驱动和 CUDA 工具包可以直接安装，简化了系统配置。
* **教育和科研的普及性**

许多高校、研究机构和培训课程都基于 Ubuntu 进行教学，这意味着很多学生和研究人员从入门阶段就开始接触 Ubuntu。这进一步巩固了 Ubuntu 在科研和深度学习领域的地位。

### 软件需求分析  
### 驱动程序和工具  
* **Nvidia驱动**  
NVIDIA驱动程序是用于让NVIDIA显卡在操作系统上正常工作的软件，它不仅提供了基本的显示功能，还包括了一些高级的功能支持，如GPU计算、3D渲染、CUDA加速等。对于深度学习任务和高性能计算（HPC），选择和配置正确的NVIDIA驱动程序至关重要。可以通过nvidia-smi来查看驱动相关信息。
* **NVCC**  
NVCC就是CUDA的编译器,可以从CUDA Toolkit的/bin目录中获取,类似于gcc就是c语言的编译器。由于程序是要经过编译器编程成可执行的二进制文件，而cuda程序有两种代码，一种是运行在cpu上的host代码，一种是运行在gpu上的device代码，所以nvcc编译器要保证两部分代码能够编译成二进制文件在不同的机器上执行。
* **CuDNN**

cuDNN的全称为NVIDIA CUDA® Deep Neural Network library，是NVIDIA专门针对深度神经网络中的基础操作而设计基于GPU的加速库。cuDNN为深度神经网络中的标准流程提供了高度优化的实现方式，例如convolution、pooling、normalization以及activation layers的前向以及后向过程。CUDA这个平台一开始并没有安装cuDNN库，当开发者们需要用到深度学习GPU加速时才安装cuDNN库，工作速度相较CPU快很多。

  


* **CUDA Toolkit**  
CUDA工具包的主要包含了CUDA-C和CUDA-C++编译器、一些科学库和实用程序库、CUDA和library API的代码示例、和一些CUDA开发工具。（通常在安装CUDA Toolkit的时候会默认安装CUDA Driver；但是我们经常只安装CUDA Driver，没有安装CUDA Toolkit，因为有时不一定用到CUDA Toolkit；  
**深度学习框架**

  


* **Tensorflow**

TensorFlow 是由 Google Brain 团队开发的一个深度学习框架，广泛应用于 Google 的研究和生产需求。其前身是一个闭源框架，被称为”DistBelief”。然而，随着深度学习的快速发展和广泛应用的需求增加，Google 在 2015 年发布了 TensorFlow 作为其开源版本。

 **常见业务用例：**

1. 图像处理和视频检测。飞机制造业巨头 Airbus 正在使用 TensorFlow 从卫星图像中提取和分析信息，为客户提供宝贵的实时信息。
2. 时间序列算法。Kakao 使用 TensorFlow 来预测网约车请求的完成率。
3. 强大的扩展能力。NERSC 使用 TensorFlow 将科学深度学习应用程序扩展至 27000 多块 NVIDIA V100 Tensor Core GPU。
4. 建模。PayPal 使用 TensorFlow 进行深度迁移学习并生成模型，能够识别复杂、临时变化的欺诈模式，同时还能通过加快客户识别流程的速度来改善合法客户的体验。
5. 文本识别。SwissCom 自定义构建的 TensorFlow 模型通过对文本进行分类以及在接听电话时确定客户意图来改善业务。

  


* **Pytorch**

PyTorch 是一款非常流行的开源深度学习框架，由 Meta AI(当时还是Facebook AI研究实验室)设计并从 AI 社区获取贡献。PyTorch 于 2016年9月发布第一个版本，迅速获得了广泛应用。

PyTorch 以 Python 为主体，但其底层实现依靠高性能的 C/C++ 程序库和 CUDA，从而使它能够充分利用 GPU 加速计算。与其他框架不同的是，PyTorch 拥有动态计算图能力，开发人员可以在训练后期修改模型结构，为研究工作提供了极大的便利。以hugging上的开源模型为例，目前大部分开源模型都是基于pytorch。

  


### 包管理和虚拟环境  
* **Anaconda**

Anaconda 是一个开源的 Python 和 R 语言发行版，专为科学计算、数据科学、机器学习和人工智能开发而设计。它集成了大量的数据科学包、环境管理工具和包管理工具，使得数据科学家和开发人员能够快速搭建开发环境并管理项目依赖。

Anaconda的核心组件

1. Conda: Anaconda 的核心工具，是一个包、依赖项和环境管理器。Conda 可以轻松安装、更新、删除和管理 Python 包及其依赖关系，还可以创建隔离的虚拟环境，避免不同项目之间的依赖冲突。
2. Python/R: Anaconda 提供了对 Python 和 R 的支持。用户可以选择使用 Python 3.x 或 2.x，也可以轻松切换到不同的 Python/R 版本。
3. 包管理: Anaconda 包含了数百个预编译的科学计算包，如 NumPy、Pandas、SciPy、scikit-learn、Matplotlib、Jupyter Notebook 等，这些包都经过优化和测试，能极大提高开发效率。
* **Pip**

pip 是 Python 的包管理工具，用于安装和管理 Python 软件包。它的全称是 "Pip Installs Packages"。pip 使得安装、卸载和管理 Python 库变得简单，通常用于从 Python Package Index (PyPI) 上下载并安装软件包。

备注：Conda和pip通常被认为几乎完全相同。虽然这两个工具的某些功能重叠，但它们设计用于不同的目的。 Pip是Python Packaging Authority推荐的用于从Python Package Index安装包的工具。 Pip安装打包为wheels或源代码分发的Python软件。后者可能要求系统安装兼容的编译器和库。

### 开发工具和库  
* **Jupyter Notebook**  
Jupyter Notebooks 是一款开源的网络应用，我们可以将其用于创建和共享代码与文档。  
其提供了一个环境，你无需离开这个环境，就可以在其中编写你的代码、运行代码、查看输出、可视化数据并查看结果。因此，这是一款可执行端到端的数据科学工作流程的便捷工具，其中包括数据清理、统计建模、构建和训练机器学习模型、可视化数据等等。
* **Git**  
一个开源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理。 也是Linus Torvalds为了帮助管理Linux内核开发而开发的一个开放源码的版本控制软件。 Torvalds 开始着手开发Git 是为了作为一种过渡方案来替代BitKeeper。许多开源项目或者模型我们可以通过git clone到本地进行部署或者开发。
* **Pycharm**

PyCharm 是一款 Python 集成开发环境，带有多种提高开发效率的工具，如调试、语法高亮等。在科学计算领域，matlab一直都是独占鳌头，但是python出现打破了这个局面，python具有的优势（众多优秀的第三方库（生态完整且强壮），较易学习，开源免费，语法优美）让其在科学计算领域也分得一杯羹，而作为python IDE中最强大的存在，pycharm也对科学计算提供了完美的支持。并且jetbrain拥有丰富的插件市场，pycharm也能享受到这些丰富的插件。



---

## 环境搭建  
### Ubuntu 22环境准备  
南京大学镜像站下载 ubuntu22 desktop：

[https://mirror.nju.edu.cn/ubuntu-releases/22.04/](https://mirror.nju.edu.cn/ubuntu-releases/22.04/) 

安装Ubuntu 22系统后更换apt的软件源。


```
sudo bash -c "cat << EOF > /etc/apt/sources.list && apt update
deb https://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiverse
deb-src https://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiverse
deb https://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiverse
deb-src https://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiverse
deb https://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiverse
deb-src https://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiverse
deb https://mirrors.aliyun.com/ubuntu/ jammy-proposed main restricted universe multiverse
deb-src https://mirrors.aliyun.com/ubuntu/ jammy-proposed main restricted universe multiverse
deb https://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiverse
deb-src https://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiverse
EOF"
```
### nVidia驱动安装与配置  
查看物理pci显示设备是否读取到Nvidia GPU

lspci | grep -i vga

![]((20241111)AI平台搭建实践_DSC/v2-b6750da0ad72a1c4c67e3384d4c4981d_1440w.png)  
安装ubuntu驱动管理


```
sudo apt install ubuntu-driver
```
自动安装推荐版本驱动，安装完重启


```
ubuntu-driver autoinstall
sudo reboot -n
```
查看nvidia 驱动版本及对应的cuda版本


```
nvidia-smi
```
![]((20241111)AI平台搭建实践_DSC/v2-7398869cb6f17fc0f1b569cbfbad7684_1440w.jpg)  
备注：nvidia-smi 的主要功能包括:

* 查询 GPU 设备信息,如型号、驱动版本、CUDA 版本等。
* 实时监控 GPU 的性能指标,如温度、功耗、内存使用率、GPU 利用率等。
* 列出当前运行在 GPU 上的进程及其资源占用情况。
* 调整 GPU 的性能状态和功耗限制。
* 设置 GPU 计算模式和 ECC(错误检查和纠正)支持。
* 启用或禁用 GPU 持久模式。

这些功能使 nvidia-smi 成为 GPU 管理和监控的必备工具,在 GPU 计算、深度学习、图形渲染等领域得到广泛应用。



---

### CUDA和cuDNN的安装  
### CUDA下载  
下载驱动对应版本：[https://developer.nvidia.com/cuda-12-4-0-download-archive](https://developer.nvidia.com/cuda-12-4-0-download-archive) 

![]((20241111)AI平台搭建实践_DSC/v2-086508dd502d9c444530924316a619fc_1440w.jpg)  

```
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda-repo-ubuntu2204-12-4-local_12.4.0-550.54.14-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2204-12-4-local_12.4.0-550.54.14-1_amd64.deb
Sudo cp /var/cuda-repo-ubuntu2204-12-4-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-4
```
检查是否配置成功


```
vim ~/.bashrc //添加cuda地址
export LD_LIBRARY_PATH=/usr/local/cuda/lib
export PATH=$PATH:/usr/local/cuda/bin
source ~/.bashrc
ncvv -V
```
![]((20241111)AI平台搭建实践_DSC/v2-dee5b6b3c5e8c912c06e31377f55fe0f_1440w.jpg)  
### cuDNN下载  
cuDNN与cuda对应版本查看:

[https://developer.nvidia.com/rdp/cudnn-archive](https://developer.nvidia.com/rdp/cudnn-archive) 

测试使用最新版本对应cuda12.x

![]((20241111)AI平台搭建实践_DSC/v2-ccfc07d3c58b02a31ff4d843d96a276a_1440w.jpg)  
在线网络下载（可能对网络环境有要求）


```
wget https://developer.download.nvidia.com/compute/cudnn/9.3.0/local_installers/cudnn-local-repo-ubuntu2204-9.3.0_1.0-1_amd64.deb
sudo dpkg -i cudnn-local-repo-ubuntu2204-9.3.0_1.0-1_amd64.deb
sudo cp /var/cudnn-local-repo-ubuntu2204-9.3.0/cudnn-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cudnn
验证cuDNN安装成功，通过运行cudnn_samples_v9 示例文件检验是否安装成功。
sudo apt-get install libfreeimage3 libfreeimage-dev
cp -r /usr/src/cudnn_samples_v9/ $HOME
cd $HOME/cudnn_samples_v9/mnistCUDNN
make clean && make
```
![]((20241111)AI平台搭建实践_DSC/v2-0018459a39d7bd69cce2070c8235e52c_1440w.jpg)  

```
./mnistCUDNN
```
![]((20241111)AI平台搭建实践_DSC/v2-59f0c879c754fc100c5b63ee9af17cbf_1440w.jpg)  
### 相关软件及环境配置  
### 安装Anaconda  
Anaconda安装包下载：

[https://www.anaconda.com/download/success](https://www.anaconda.com/download/success) 

安装Anaconda


```
bash ./Anaconda3-2024.06-1-Linux-x86_64.sh
```
![]((20241111)AI平台搭建实践_DSC/v2-7036d8ae776bf646602b453868789d05_1440w.jpg)  
设置conda多用户使用，创建用户组


```
groupadd anaconda
```
将需要使用conda的用户添加到用户组中。可以使用以下命令将用户添加到用户组：


```
usermod -a -G anaconda username
```
移交目录管理权


```
chgrp -R anaconda /PATH/anaconda3
```
设置读写权限


```
chmod 770 -R /PATH/anaconda3
```
设置继承组和子目录组继承


```
chmod g+s /PATH/anaconda3
chmod -R g+s /PATH/anaconda3
```
配置环境变量


```
export PATH=/PATH/anaconda3/bin:$PATH
export CONDA_ROOT=/PATH/anaconda3
source ~/.bashrc
```
创建环境


```
conda create -n xxxx python=<python版本号>
```
![]((20241111)AI平台搭建实践_DSC/v2-e54fa99bc18421cfb7fbd49e00252d8b_1440w.jpg)  
进入conda环境内


```
source activate xxxx
```
![]((20241111)AI平台搭建实践_DSC/v2-9e9dfc2a88228c28ffc6b544cadac1ca_1440w.jpg)  
更换conda源为国内源


```
Vim ~/.condarc

channels:
  - defaults
show_channel_urls: true
default_channels:
  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main
  - https://mirrors.ustc.edu.cn/anaconda/pkgs/r
  - https://mirrors.ustc.edu.cn/anaconda/pkgs/msys2
custom_channels:
  conda-forge: https://mirrors.ustc.edu.cn/anaconda/cloud
  pytorch: https://mirrors.ustc.edu.cn/anaconda/cloud
```
清除缓存


```
conda clean -i
```
### Pip设置  
更换pip为阿里源


```
pip config set global.index-url https://mirrors.aliyun.com/pypi/simple
pip config set install.trusted-host mirrors.aliyun.com
```


---

## 总结  
本文是初衷是通过4-5K的预算，搭建一个支持多GPU的硬件平台，运行大模型训练/推理。

期间学习了相关硬件技术如CPU技术路线（Intel vs AMD)，主板技术(PCIE等），在网上寻找合适的品牌服务器主板，研究各种主流模型对显存的要求，在预算有限的情况下选择了曾经的一代卡皇Nvidia 2080TI（魔改大显存22G），在Vmware 和 KVM平台上进行了开发运行环境的搭建。

* 满意点：
1. 追风者614PC ，高端大气上档次，硬盘位最大可以做到4个2.5+10个3.5，兼做NAS；
2. 联想RD450X主板，支持IPMI远程维护，板载的10个SATA硬盘接口已满足NAS需求，配备2个万兆电口实现高带宽，主板刷了补丁后，E5-2666v3可以跑满；
3. 魔改的2080Ti 22G是个惊喜，估计PCB板是全新的抄板设计，主处理核心应该是利旧（矿卡），散热器也是全新（三风扇）；
* 踩坑点：
1. 虽然联想RD450X是一个不错的主板，但没想到在PCIE 3.0 X16的布局有缺陷，如果再有选择，会改选超微7048GR-TR/浪潮的NF5568M4主板，正规的4 GPU主板;
2. 利旧使用的850Watts 电源功率在单GPU场景下没有问题，但双GPU卡（22G\*2 ,Nvlink）最好配1250Watts电源；
3. 原本的设计是Vmware 做底层，运行各种虚拟机（包括网络仿真EVE-NG ，NAS系统，并诱骗直通Nvidia 2080TI 到ubuntu 做AI推理。但Vmware6.7/7/8都有显卡掉驱动的情况，不得以只能底层安装ubuntu 22，用KVM来做虚拟化底座。
* 遗憾点

基于硬件平台的限制，无法通过Nvlink体验到2卡，4卡互联，去尝试44G/88G显存并行运算，（备注：22G\*2 or 22G\*4，多卡互联不能显存叠加,多个显卡增加batch\_size实际上是进行了[https://zhida.zhihu.com/search?q=数据并行&zhida\_source=entity&is\_preview=1](https://zhida.zhihu.com/search?q=数据并行&zhida\_source=entity&is\_preview=1)计算，即将大batch\_size拆成了多个小的batch\_size进行运算，最后再进行聚合运算)，Meta 的Llama 3.1 开源大语言模型，提供 8B、70B 及 405B 参数版本 ，无法体验到405B，也算是意难平吧。

