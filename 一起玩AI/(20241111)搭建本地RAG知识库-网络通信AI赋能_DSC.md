# 搭建本地RAG知识库-网络通信AI赋能

 **Author:** [DSC]

 **Link:** [https://zhuanlan.zhihu.com/p/6274636667]

## 引言  
网络技术的发展历程充满了变革与创新。从早期的计算机网络互联，到如今复杂的全球互联网架构，网络通信技术经历了从局域到广域，从固定到移动，从人工管理到自动化运维的跨越式发展。最初，网络依赖人工配置和管理，随着规模的扩大和需求的增长，网络自动化工具逐渐应运而生，实现了网络配置、流量监控、故障排除等方面的部分自动化。 

AI发展的迅猛，各行各业都在努力与AI靠拢，企图乘上这趟快车，同时AI的发展也让大家看到了广阔的前景，这并不是局限某个行业的红利，它已经被广泛应用于各个领域，包括金融、医疗、物流、智能制造等等。AI也开始逐渐融入网络通信领域，为网络运营与管理提供了全新的解决方案。AI不仅能够通过机器学习和数据分析对网络进行自我优化和调整，还可以实现智能化的网络运维，如自动故障检测、流量预测、智能路由选择等，极大提升了网络的效率与可靠性。AI的爆发式发展，尤其是深度学习和自然语言处理的突破，赋予了网络管理系统更高层次的智能，使网络系统能够在面对复杂环境和海量数据时，做出更加精准和自适应的决策。

基于深度学习的AI网络应用一直在发展研究，如利用RNN循环神经网络(Recurrent Neural Networks)与LSTM长短时记忆网络（Long Short-Term Memory）进行网络故障预测或网络安全攻击预测。

但笔者认为相关技术用于推广普及使用仍有一定的的难度，AI相关应用最关键的点在于数据集，这直接关乎到模型的训练效果，而现实网络当中，首先是高质量、标注好的大数据集构建难度大且成本高，尤其是在涉及敏感信息或领域。而且也不可能随便共享复用，数据隐私性是关键问题。而且目前常规的网络运维监控通常使用zabbix及Prometheus采集网络设备信息，信息繁杂，噪声过大，难以直接用作训练集，而各自网络环境不同，数据清洗仍是一大考验。其次是各网络厂商之间壁垒较高，系统，命令格式，硬件规格差异性较大，私有特性与协议也各有不同，较难以一个规范化标准来推广应用。

2023年AI的爆发也得益于之前长时间发展的积累，ChatGPT的横空出世第一次让普罗大众直观的感受到AI所带来的冲击，GPT模型是基于Transformer结构的一种语言模型，Transformer模型是现代深度学习的前沿技术之一。它摆脱了传统循环神经网络(RNN)和卷积神经网络(CNN)的局限性，引入了注意力机制，以其卓越的文本处理和生成能力，引领了自然语言处理（NLP）的革命。如检索增强生成（RAG）技术的应用。

通过RAG技术来搭建知识库，也是目前最容易落地的AI项目，笔者认为其在AI网络应用中仍有发展空间，因为知识库还有良好的兼容性与其他技术相结合，不仅仅是对于网工的帮助，也能为整个企业带来广泛的价值。

设想一个场景，通过RAG知识库的本地部署，针对内部资料进行训练首先可以确保一个数据的保密性与安全性。其次，可以分门别类建立多个小型的知识库如售前可以是一些产品价目表，招标参数，产品详细介绍，各行业相关解决方案材料，售后是一些产品配置手册，命令行指导手册等等，市场部可以设置知识库为公司相关介绍，相关产品彩页，市场活动资料等等。建立不同的知识库可以通过单点登录设置分权分域，而且可以降低模型推理所需的硬件成本。建立知识库后可以通过聊天助手或AI Agent对接，通过自然语言聊天实现与人的交互。

**业界目前在AI知识库方面的相关应用如下：**

* Gartner（全球最具权威的IT研究与顾问咨询公司） ：  
近期发布了一份有关《在客户体验中应用人工智能（Gartner Newsletter: Leveraging AI in CX）》的报告，其中就包含多项研究成果以及如何让人工智能成为企业的战略优势。  
经过大量的产品文档、手册和知识库文章的培训，利用生成式 AI 的强大功能，提高RAG知识库的准确性和上下情境来理解、解释和响应客户的查询。让聊天助手或AI Agent对产品实现深入的了解，这样即便是面对复杂的问题，也能提供即时、准确和与上下文相关的解答，从而提升客户体验(Customer Experience)

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-d14431ac0e95a7062647be4b9e5e58ba_1440w.jpg)  
* Grafana 的部署案例  
文档帮助助手：

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-8c91daa33bd632c4f5ecd7fe8a237b1b_1440w.jpg)  
* 美团智能客服效果示范：

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-48324ba2967025f11ce431d47abc9989_1440w.jpg)  


---

**术语表**



| 序号 | 名称 | 描述 |
| --- | --- | --- |
| 1 | RAG(Retrieval-Augmented-Generation) | 检索增强生成 |
| 2 | Embedding | 嵌入处理 |
| 3 | Rerank | 重排序 |
| 4 | LLM | 大型语言模型 |

  
**配置环境**



| 序号 | 名称 | 详细信息 |
| --- | --- | --- |
| 1 | GPU | NVidia 2080TI-22G |
| 2 | Embedding模型 | bge-m3 |
| 3 | Anaconda3 | Python3.12 |
| 4 | Rerank模型 | bge-reranker-large |
| 5 | LLM模型 | Llama3.1:8b |

**部署流程**  


![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-a4401ffd765b5f03bb88a166bbcc997a_1440w.jpg)  


---

  
**RAG(Retrieval-Augmented-Generation)**  
RAG 是一种结合信息检索与文本生成的架构，旨在利用外部知识库来增强生成模型的能力。它首先从一个知识库中检索相关文档，然后使用生成模型（如 GPT 或 BERT）结合这些文档的信息来生成答案或文本。这种方法可以显著提升生成的准确性和丰富度，因为它将大量外部知识融入到生成过程中。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-b752385151a97b4055c5fbeb4be92695_1440w.jpg)  
  
RAG 的工作流程对应其名，检索，增强，生成：

1. 输入查询：用户输入一个查询，比如一个问题或任务描述。
2. 查询嵌入：将查询通过 Embedding 模型转换为一个向量表示嵌入向量数据库。
3. 文档检索：在预先嵌入的知识库中，通过查询向量与文档向量的相似度，检索出最相关的文档。
4. 生成回答：使用LLM生成模型结合检索到的文档内容生成最终的回答或文本输出。



---

### 开源AI平台Dify介绍  
官网上定义自身为生成式 AI 应用创新引擎，一个开源的 LLM 应用开发平台。提供从 Agent 构建到 AI workflow 编排、RAG 检索、模型管理等能力，轻松构建和运营生成式 AI 原生应用。并且毫不避讳的表示“比 LangChain 更易用”

官网：[http://dify.ai/](http://dify.ai/)

备注：LangChain是一个开源库，用于构建和运行基于语言模型的应用程序。它是由语言模型（如GPT）的用户和开发者设计的，旨在简化大型语言模型（LLM）的使用和集成过程。但是他更偏向于底层代码化设计，笔者认为更针对与开发者。

Dify有公有云解决方案与本地化社区版

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-37227cc516bfa7cddf08cc675ad57b3b_1440w.jpg)  
### 系统平台搭建  
目前DIfy release稳定版本为0.11.0

推荐使用Docker compose来部署，一键操作，快速入手


```
git clone https://github.com/langgenius/dify.git
```
设置代理（可选）：


```
git config --global http.proxy http://ipaddress:port
git config --global https.proxy https://ipaddress:port
```
查看已有代理


```
git config --global -l
```
或使用单次代理


```
git clone -c http.proxy="http://ipaddress:port" https://github.com/langgenius/dify.git
```
进入 Dify 源代码的 Docker 目录


```
cd dify/docker
```
复制环境配置文件


```
cp .env.example .env
```
启动Dify


```
docker compose up -d
```
目前需设置Docker代理或加速镜像

-加速镜像：


```
sudo mkdir -p /etc/docker

sudo tee /etc/docker/daemon.json <<EOF{
    "registry-mirrors": 
        "https://docker.anyhub.us.kg",
        "https://dockerhub.jobcher.com",
        "https://dockerhub.icu",
        "docker.1panel.live",
"https://docker.rainbond.cc"，
    ]
}
EOF

sudo systemctl daemon-reload
sudo systemctl restart docker
```
代理设置：


```
sudo mkdir -p /etc/docker
sudo vim /etc/docker/daemon.json

{
"proxies": {
"http-proxy": "http://ipaddress:port",
"https-proxy": "https://ipaddress:port",
"no-proxy": "*.xxx.com,127.0.0.0/8"
}
}

sudo systemctl daemon-reload
sudo systemctl restart docker
```
可以看到服务已启动用到的一些数据库以及中间件。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-f247c319a6ee278162b1925d9c51db73_1440w.jpg)  


---

WEB输入IP地址，默认开放80端口，监听0.0.0.0，初次登陆设置管理员账号即可使用

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-3bf29c4778e88fe43359b59443ac1d8a_1440w.jpg)  
升级DIfy操作：


```
备份您的自定义 docker-compose YAML 文件（可选）：
cd docker
cp docker-compose.yaml docker-compose.yaml.$(date +%s).bak
从主分支获取最新代码：
git checkout main
git pull origin main
```
1. 停止服务，命令请在docker目录下执行，备份数据重新开启服务


```
docker compose down
tar -cvf volumes-$(date +%s).tgz volumes
docker compose up -d
```


---

### 打造个性化聊天助手  
在打造个性化的AI之前需要调用第三方或者部署本地大模型。可以在web界面直接接入相关模型。从简单快速的入手，先调用第三方API尝试生成自己的聊天助手。将RAG集成聊天助手，可以通过聊天助手来接收查询与输出检索增强的内容。

在设置的模型提供商处，有预设的接口方便直接调用第三方大模型的API进行接入。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-bcea7cd78e17fc02ede9f03da8d7ca6c_1440w.jpg)  
![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-b612d5b09b3c77cbd0587aaef02ef8d9_1440w.jpg)  
调用第三方AI api：可以从国产及免费的几款模型进行尝试。如Kimi、GLM（智谱清言）、Spark（讯飞星火）。

* Moonshot AI (Kimi)：  
点击获取API key会直接跳转Moonshot AI开发者平台生成API。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-5cc2367029199da9efdde494cdc39a35_1440w.gif)  
* 讯飞星火：

可以领取Spark4.0 Ultra/Spark3.5 Max等等试用token进行调用

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-7c20128b4db0d1205e89d7885053f168_1440w.jpg)  
或使用免费的Spark Lite

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-24e4f7f85b3d1540d8fa01b85340d9d4_1440w.jpg)  
讯飞星火API相关配置

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-73cc3c5a76f7a4373921c5abd23db511_1440w.jpg)  
界面上方选项选择工作室，基础编排一个聊天助手。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-4eba165c7f569bedaf8358b8ab4b2adf_1440w.jpg)  
备注：关于聊天助手与Agent（AI 智能体），聊天助手（Chatbot）基于 LLM 构建对话式交互的助手，入门级及市面上大部分AI都是基于此，注重与人交互，需要人为主动输入触发进行提供信息、回答问题、解决疑惑等。AI Agent被定义为能够主动感知环境、进行决策和执行动作的智能实体。具备自主性和自适应性，在特定任务或领域中能够自主地进行学习和改进。笔者不认为这是两个独立的个体，Agent可以看作更高层次的发展方向，基础与前提仍需理解人类的各项指令和意图，再去实现更多的职责，当然Agent仍需有自主性，可以在没有人类干预的情况下运行，独立做出决策并执行这些决策。如无人驾驶汽车或无人机。

开始编排聊天助手，在提示词中可以描述你生成自己的聊天助手类型，

如IT专家，网络安全专家。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-25e96a4660e7ec6b418ae7c2bfbe2f99_1440w.jpg)  
如果没想好生成什么类型的聊天助手不妨看看这个Github 110K的项目

（awesome-chatgpt-prompts）:[https://github.com/f/awesome-chatgpt-prompts ](https://github.com/f/awesome-chatgpt-prompts )

这个集合提供了非常多AI助手提示词（prompt）。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-4d12ae6067594107c8b04e49151dd394_1440w.jpg)  
添加更多的功能，如自定义聊天助手的开场白。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-5e5832e96a7c9d7260ce68f9a47d742f_1440w.jpg)  


---

右侧选择聊天助手使用的模型

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-eb3349fbc5a5388d4bcfae0eb0756e48_1440w.jpg)  
最后点击发布-更新-运行即可。

### 本地LLM模型搭建  
搭建企业内部知识库，基本离不开本地生成式模型的搭建，本地离线模型可以保证内部数据的安全性，通过大语言模型用于精确回复用户问题

### Ollama  
Ollama 是一个工具和平台，专门用于运行和管理大型语言模型（LLM）。它简化了模型的部署和使用，特别适用于那些没有太多资源或技术背景的用户，使他们能够轻松访问和操作强大的 AI 模型。Ollama 允许用户通过简化的界面和 API 来加载和使用模型，避免了手动配置和部署的复杂性。

官网：[https://ollama.com/](https://ollama.com/) 

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-8fe4d953036e97fce2dc4ae0c5b268cd_1440w.jpg)  
### 模型选择  
本地模型选择可以参照hugging face开源大模型排行open\_llm\_leaderboard

[https://huggingface.co/spaces/open-llm-leaderboard/open\_llm\_leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open\_llm\_leaderboard) 

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-d9c62844c667481246b15e1d7bec5564_1440w.jpg)  
下面是Ollama模型库常见模型，即支持通过Ollama安装的模型。



| Model | Parameters | Size | Download |
| --- | --- | --- | --- |
| Llama 3.1 | 8B | 4.7GB | ollama run llama3.1 |
| Llama 3.1 | 70B | 40GB | ollama run llama3.1:70b |
| Llama 3.1 | 405B | 231GB | ollama run llama3.1:405b |
| Phi 3 Mini | 3.8B | 2.3GB | ollama run phi3 |
| Phi 3 Medium | 14B | 7.9GB | ollama run phi3:medium |
| Gemma 2 | 2B | 1.6GB | ollama run gemma2:2b |
| Gemma 2 | 9B | 5.5GB | ollama run gemma2 |
| Gemma 2 | 27B | 16GB | ollama run gemma2:27b |
| Mistral | 7B | 4.1GB | ollama run mistral |
| Moondream 2 | 1.4B | 829MB | ollama run moondream |
| Neural Chat | 7B | 4.1GB | ollama run neural-chat |
| Starling | 7B | 4.1GB | ollama run starling-lm |
| Code Llama | 7B | 3.8GB | ollama run codellama |
| Llama 2 Uncensored | 7B | 3.8GB | ollama run llama2-uncensored |
| LLaVA | 7B | 4.5GB | ollama run llava |
| Solar | 10.7B | 6.1GB | ollama run solar |

测试使用GPU: 2080Ti -22G，所以要使用22G以下显存的模型部署。这里推荐Llama3.1-8B，Gemma2-9B，以及中文友好的清华智谱清言GLM4-9B，这些模型也刚好满足消费甜品级显卡需求，如RTX3060 8/12G，RTX3060TI 8G，RTX4060 8G，RTX4060TI 8/16G。不推荐使用AMD,没有CUDA。

Llama3.1

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-febbca217f5bb9a9b144b12915643c64_1440w.jpg)  
![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-f26f669c3d3cddf46d2258accb4a2262_1440w.jpg)  
除了基础模型外选择view more会出现更多的模型，如Instruct，text，Chat之类的为遵循指令或完成特定任务而设计和优化的模型。

* **模型用途**：
1. Text：通常是用于生成一般性的文本，例如文章写作、文本补全等。这类模型可能会有较为开放的文本生成能力。
2. Instruct：这种模型是专门用来处理指令或任务导向的输入（例如问答、步骤指导等），通常在遵循指令方面表现更好。
3. Chat：这种模型针对对话进行了优化，适用于聊天机器人、实时问答等场景。
* **模型精度**：
1. fp16（16位浮点数）模型：这种模型在计算时使用的是16位浮点数，通常比32位浮点数模型更节省内存，同时在精度损失不大的情况下提升性能。适合在内存有限的环境中使用。
2. int8 或 q4\_k\_m：这些模型是量化过的模型，通过减少模型参数的位数来降低内存占用和计算需求，通常用于资源受限的设备，但可能会牺牲一些精度。大内存模型的高量化模型也会比小内存模型性能强一些。
* **性能与资源需求**：
1. 如果希望在设备资源有限（如边缘计算设备）下运行，选择更低内存占用的量化模型（如q4\_k\_m）是比较好的选择。量化程度越高，性能需求更低，模型的能力越差，如q2\_k这种数字越低所需性能越低，随之性能也会越差。
2. 如果资源充足且希望获得最高的模型表现，选择精度更高的模型如fp16。



---

### 模型部署  
选择基础版本部署

Llama3.1：


```
ollama run llama3.1:8b
```
同样在模型提供商处添加模型

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-70e09a96ba18b8cb06e4369b172c06c0_1440w.jpg)  
GLM4：


```
ollama run glm4:9b
```
### 本地Embedding模型搭建  
Embedding 模型是一种用于将高维数据（通常是离散的文本、图像或其他符号数据）转换为低维连续向量表示的模型。Embedding 在自然语言处理 (NLP) 和计算机视觉等领域中应用广泛，主要目的是将复杂的数据结构映射到一个连续的向量空间，以便于进行相似性计算、聚类、分类等任务。

### Embedding 模型在 RAG 中的作用  
Embedding 模型在 RAG 系统中主要用于以下几个方面：

1. **文档检索（Document Retrieval）**：   
在 RAG 中，生成答案的第一步是从知识库中检索相关文档。Embedding 模型通过将文本（如查询和文档）转换为向量表示，然后在向量空间中进行相似性搜索，来找到最相关的文档。这种向量表示捕捉了文本的语义信息，使得检索更加精确。
2. **查询与文档的匹配**：   
通过使用 Embedding 模型，RAG 系统可以计算查询和知识库中文档的向量相似度，选择最相关的文档进行进一步处理。通常使用余弦相似度或其他距离度量方法来评估查询与文档之间的相关性。
3. **构建知识库**：   
知识库中的文档或信息通常也会使用 Embedding 模型进行预处理，生成嵌入向量。这些向量存储在数据库中，方便快速检索。知识库的规模可以很大，包括百科全书、技术文档、产品手册等多种类型的数据。
4. **增强生成模型**：

一旦检索到相关文档，生成模型将这些文档的信息与输入的查询结合起来，生成更加丰富和准确的文本输出。Embedding 模型通过确保检索到的文档与查询高度相关，间接提高了生成模型的表现。



---

### 如何选择Embedding模型  
MTEB (Massive Text Embedding Benchmark) 是一个用于评估文本嵌入模型（Text Embedding Models）性能的基准测试平台。它通过大量的任务和数据集，对不同的文本嵌入模型进行全面的性能评估。MTEB 的设计目的是提供一个统一的标准来比较和分析不同文本嵌入技术的优劣，使研究者和开发者能够更好地选择和优化模型。

### 模型部署  
根据hugging face网站上的排名

[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard) 

可以选择用于中文Embedding的模型，如bge-m3

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-9992f1b0bb7be40c599df3b439a3991c_1440w.jpg)  
同样在Ollama 找到模型。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-db79899b03ecc4919a1686ae7ceefa5b_1440w.jpg)  

```
ollama run bge-m3
```
在dify模型供应商内添加，选择Text Embedding模型

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-c74229997b881a72b7c511743dbf5421_1440w.jpg)  
可以在系统模型设置内设置默认模型

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-cec823d53369e587bbdaa206af97ea6a_1440w.jpg)  
### 本地Rerank模型搭建  
### 什么是Rerank模型  
Rerank模型是用于信息检索和排序任务的模型。它的主要作用是在初步检索或排序后，对候选结果进行重新排序，以提高最终的结果质量。通常，Rerank模型会对初步生成的一组文档、网页、答案等候选集进行更细致的评估和评分，然后按照新的评分顺序排列。

Rerank技术常与Retriever-Augmented Generation (RAG)等技术相结合，用于知识检索、文档总结和回答问题的任务中。在RAG框架中，retriever负责找到相关文档，而reranker则负责对找到的文档进行重新排序，确保生成内容基于最优质的信息。

### Embedding与Rerank模型的分工  
Rerank模型的主要任务是对Embedding模型初步筛选出的候选集进行重新排序，确保最相关的结果排在最前面。它通常基于更复杂的语义分析，评估候选文档和查询之间的深层次匹配关系。在Rerank阶段，模型会分析查询与候选文档之间的上下文、语义关系等信息。它可以使用诸如BERT/GPT等预训练语言模型来捕捉更细腻的语义和句子间的关系，从而对初步候选文档进行更精确的评分与排序。Rerank模型一般比Embedding模型计算更复杂，通常需要更多的计算资源，因此适合处理Embedding模型初步检索后的数据。

* **初步检索（Embedding模型）**：用户输入查询后，Embedding模型首先将查询和文档表示为向量，然后通过向量相似度计算，快速从大规模数据集中筛选出若干个候选文档或候选答案。
* **重新排序（Rerank模型）**：在得到初步候选集后，Rerank模型进一步分析这些候选文档或答案与查询之间的精确匹配程度，并根据复杂的语义关系重新打分，对候选集进行排序。



---

### 模型部署  
选择Text Embedding Inference，可以从红框名称的下方看到支持的RERANK模型类型。

github：[https://github.com/huggingface/text-embeddings-inference](https://github.com/huggingface/text-embeddings-inference) 

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-ae1fe5fb01654e010b79b6f7b33a5da0_1440w.jpg)  
采用Docker部署模型，Docker调用GPU，需先安装nvidia-container-toolkits

配置生产存储库：


```
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
```
从存储库更新包列表与安装NVIDIA Container Toolkit 


```
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
```
一键部署BAAI/bge-reranker-large模型。


```
Docker run --gpus all -p 18080:80 -v /reranker:/data --pull always ghcr.io/huggingface/text-embeddings-inference:turing-1.5 --model-id BAAI/bge-reranker-large
```
模型地址参照下方表格



| Architecture | Image |
| --- | --- |
| CPU | [http://ghcr.io/huggingface/text-embeddings-inference:cpu-1.5](http://ghcr.io/huggingface/text-embeddings-inference:cpu-1.5) |
| Volta | NOT SUPPORTED |
| Turing (T4, RTX 2000 series, ...) | [http://ghcr.io/huggingface/text-embeddings-inference:turing-1.5](http://ghcr.io/huggingface/text-embeddings-inference:turing-1.5) (experimental) |
| Ampere 80 (A100, A30) | [http://ghcr.io/huggingface/text-embeddings-inference:1.5](http://ghcr.io/huggingface/text-embeddings-inference:1.5) |
| Ampere 86 (A10, A40, ...) | [http://ghcr.io/huggingface/text-embeddings-inference:86-1.5](http://ghcr.io/huggingface/text-embeddings-inference:86-1.5) |
| Ada Lovelace (RTX 4000 series, ...) | [http://ghcr.io/huggingface/text-embeddings-inference:89-1.5](http://ghcr.io/huggingface/text-embeddings-inference:89-1.5) |
| Hopper (H100) | [http://ghcr.io/huggingface/text-embeddings-inference:hopper-1.5](http://ghcr.io/huggingface/text-embeddings-inference:hopper-1.5) (experimental) |

部署后查看镜像是否正常启动

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-f4b22cfa099d823d09d369f3ee5620ec_1440w.png)  
接入Dify的模型提供商

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-fb5d9fa30e332253e6e5bbb389ad4503_1440w.jpg)  
### 知识库构建  
### 文档导入  
选择知识库，创建知识库，支持多种格式文档，或者同步notion内的笔记与外部Web网站内容。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-7a5abff1879b8dbf407caedf1e1acac8_1440w.jpg)  
### 数据分段与清洗  
上传文档后会进行文本的分段与清洗，选择自动分段即可，高质量索引，下方会计算预估消耗的token数量，因为是本地部署无需担心数量限制（这也是本地部署的优势之一）。可以采用Q&A模式，官方介绍说明：在知识库上传文档时，系统将对文本进行分段，总结后为每分段生成 Q&A 匹配对。与高质量与经济模式中所采用的「Q to P」（用户问题匹配文本段落）策略不同，QA 模式采用 「Q to Q」（问题匹配问题）策略。这是因为问题文本通常是是具有完整语法结构的自然语言，Q to Q 的模式会令语意和匹配更加清晰，并同时满足一些高频和高相似度问题的提问场景。当用户提问时，系统会找出与之最相似的问题，然后返回对应的分段作为答案。这种方式更加精确，因为它直接针对用户问题进行匹配，可以更准确地帮助用户检索真正需要的信息。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-e30ff21aae48e2d98e9c5c22edf1fdfb_1440w.jpg)  
检索方式选择推荐的混合索引并配合本地部署的Rerank模型。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-e19f539ba08b2c243ba1b9da8b3809c1_1440w.jpg)  
完成数据分段清洗后可以在知识库内查看分段的情况，共5732个段落，以Q&A格式呈现。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-3ae38e895c9331ab525286d8c44c9d58_1440w.jpg)  
### 大模型集成知识库  
在工作室创建聊天助手，并在上下文中添加刚刚创建的知识库。并在右上角选择进行语义识别，对话的模型。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-901a67e43e846e42cc4bf81a5c33942d_1440w.jpg)  


---

支持添加引用多个知识库，目前使用多路召回模式，回答提问时可能设及到多个知识库，如配置方法加配置命令，那么会从不同的知识库同时召回进行回答，而非多个知识库中选其中一个进行召回。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-10a316efcf06cb0b63945d32adf0a94d_1440w.jpg)  
点击添加功能，可以开启如对话开场白，下一步建议，及引用归属。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-bc73621e439587ac2b04ccc4c6a83afc_1440w.jpg)  


---

引用及归属开启后可以看到回答来源知识库

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-33c7ea53b46a6b048d40f01bda3b73b1_1440w.jpg)  
自定义添加更多功能

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-c2217f71a46e03ffdcb8db93c7d4d606_1440w.jpg)  
### 知识库模型监控  
在聊天助手内开启监控可以对该应用进行监控

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-20e874daf78f7b86015a5e7c68520875_1440w.jpg)  
可以通过部署Langfuse或LangSmith来监控模型与微调， 

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-af9db28d9daf4d6c591adbae136f3668_1440w.jpg)  
### 部署Langfuse  
Langfuse 是一个开源的 LLM 工程平台，可以帮助团队协作调试、分析和迭代他们的应用程序。

Github: [https://github.com/langfuse/](https://github.com/langfuse/) 

Docker一键部署


```
git clone https://github.com/langfuse/langfuse.git
cd langfuse
docker compose up -d
```
默认监听0.0.0.0，端口3000

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-584d07c7c2c4f704ff20fa7b5dc43656_1440w.jpg)  
创建组织-新建应用会得到相关的公私密钥及服务地址



---

在聊天助手应用内配置对接

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-56e8dd96cf25131c3ba612fa081349ac_1440w.jpg)  
### 监控分析  
开启追踪可以看到每次对话问答的详细情况。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-32137450432d18e9b12c7566c2b847cd_1440w.jpg)  
* **ID**：唯一标识每个追踪记录的ID。
* **Timestamp**：每条记录的时间戳，表示何时生成或触发了该条记录。
* **Name**：操作的名称，这里大部分是“message”，表示生成了一条消息。
* **User**：用户ID或相关信息，用于标识该操作由哪个用户触发。在这个截图中，多个记录的用户ID是类似 "ea3a6533-fc30-..." 的字符串。
* **Session**：会话ID，表示每条记录属于哪个会话。
* **Latency**：每次请求或操作的延迟时间，以毫秒（ms）为单位，表示该操作的响应速度。
* **Usage**：资源使用量的变化。格式是“初始使用量 → 当前使用量（总和 ∑）”，例如“1,500 → 283 (∑ 1,783)”表示资源从1,500 token减少到283 token，总共使用了1,783 token。
* **Scores**：性能或质量的评分。
* **Total Cost**：总费用，当前所有记录都显示为$0.00，表示没有费用或者该操作不涉及费用。
* **Tags**：附加的标签，用于标识记录的类别或用途，如 "chat" 和 "message"。

可以查看具体trace内容

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-7bedd91f067dcce0deaae4d89c7d7f64_1440w.jpg)  
## 应用效果演示  
### 售前应用场景：CPL产品问答  
 当售前收到项目需求后，与AI Agent进行沟通，将设备的相关需求参数输入Agent，Agent根据知识库给出过往项目组网与解决方案进行参考，当人为确定组网架构时，Agent通过查询知识库匹配并给出推荐产品与相关价格。

此处不展示图片。

### 售后应用场景：产品详细配置问答  
在售前确认相关解决方案的同时，Agent根据组网架构与产品给出建议性配置于售后，如OSPF、VLAN划分，堆叠配置或VRRP。售后可以进行进一步沟通与明确具体需求让Agent从知识库内调用相关的产品配置命令并输出，用于ZTP配置或一键导入预配进行开局。

![]((20241111)搭建本地RAG知识库-网络通信AI赋能_DSC/v2-c2fb0b0e524f0984ca1aeeed21d13008_1440w.jpg)  
### 市场营销应用场景：公司介绍、经典案例  
可以将公司介绍与简单产品介绍生成知识库，以聊天助手调用知识库的形式嵌入到公司官网内，可供客户浏览网页时进行初步了解。

